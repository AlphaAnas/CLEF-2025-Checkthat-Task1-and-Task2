{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"0c297db8641d4100b1a14ff9dc83a22e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58163c6d1c6b465a84e9446d86032c08","placeholder":"​","style":"IPY_MODEL_b1cf825e35a7412686ac83f95523e486","value":" 50/50 [00:00&lt;00:00, 316.92 examples/s]"}},"17b0cdd649ae416f9ab47f98f96bde43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27b51a78e2534445b26569403117d721":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d325843bb894859bc3be2b6133cab80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d8f70c8fed443fc9913128b6ec07f73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f9c68a813f34a0d9a31f1a0b5614cec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d8f70c8fed443fc9913128b6ec07f73","placeholder":"​","style":"IPY_MODEL_8f8d81bce67d4ff597e2214b803f85e2","value":"Map: 100%"}},"58163c6d1c6b465a84e9446d86032c08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58ed0a8ed51748b19da8a7090471c10c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f9c68a813f34a0d9a31f1a0b5614cec","IPY_MODEL_cd52984688f748eab9506d5a45c71fe7","IPY_MODEL_bc60585169334c95b934492b370ad08a"],"layout":"IPY_MODEL_71b80d5a96fa436cad5cec36a785b299"}},"6cde57f4c7644563b945b78126afa8b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71b80d5a96fa436cad5cec36a785b299":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f8d81bce67d4ff597e2214b803f85e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90de8bb7d172475497cca7a0d49335c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d325843bb894859bc3be2b6133cab80","placeholder":"​","style":"IPY_MODEL_c323e758408c4ddb9ec66f682183e7d9","value":"Map: 100%"}},"b1cf825e35a7412686ac83f95523e486":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1f2ac95d10842d9bda6948f8df7b44f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_90de8bb7d172475497cca7a0d49335c2","IPY_MODEL_dcac984e304e4a0ca4e13fe07825d7e4","IPY_MODEL_0c297db8641d4100b1a14ff9dc83a22e"],"layout":"IPY_MODEL_eae53927bec540529741bd422e241f83"}},"bc60585169334c95b934492b370ad08a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7dec48e3bfc46a199fb96b607d17b78","placeholder":"​","style":"IPY_MODEL_eedf0458dcab401da3c4983d7ce23885","value":" 102/102 [00:00&lt;00:00, 264.07 examples/s]"}},"c323e758408c4ddb9ec66f682183e7d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3fdeab71a5246fa932024199b3000b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd52984688f748eab9506d5a45c71fe7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3fdeab71a5246fa932024199b3000b6","max":102,"min":0,"orientation":"horizontal","style":"IPY_MODEL_27b51a78e2534445b26569403117d721","value":102}},"dcac984e304e4a0ca4e13fe07825d7e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_17b0cdd649ae416f9ab47f98f96bde43","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6cde57f4c7644563b945b78126afa8b4","value":50}},"eae53927bec540529741bd422e241f83":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eedf0458dcab401da3c4983d7ce23885":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7dec48e3bfc46a199fb96b607d17b78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11146713,"sourceType":"datasetVersion","datasetId":6953759},{"sourceId":11463324,"sourceType":"datasetVersion","datasetId":7183271},{"sourceId":353647,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":295028,"modelId":315638}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:19:43.236477Z","iopub.execute_input":"2025-05-11T02:19:43.237105Z","iopub.status.idle":"2025-05-11T02:19:43.241106Z","shell.execute_reply.started":"2025-05-11T02:19:43.237072Z","shell.execute_reply":"2025-05-11T02:19:43.240244Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\n\n# List contents of the current working directory\nprint(\"Contents of the current working directory:\")\nos.system('ls -lh /kaggle/working')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:19:43.244003Z","iopub.execute_input":"2025-05-11T02:19:43.244255Z","iopub.status.idle":"2025-05-11T02:19:43.398722Z","shell.execute_reply.started":"2025-05-11T02:19:43.244232Z","shell.execute_reply":"2025-05-11T02:19:43.397866Z"}},"outputs":[{"name":"stdout","text":"Contents of the current working directory:\ntotal 8.0K\ndrwxr-xr-x 5 root root 4.0K May 11 02:18 saved-models-flan-t5-base\ndrwxr-xr-x 3 root root 4.0K May 11 02:18 wandb\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import shutil\nshutil.rmtree('/kaggle/working', ignore_errors=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:19:43.400027Z","iopub.execute_input":"2025-05-11T02:19:43.400347Z","iopub.status.idle":"2025-05-11T02:19:44.739734Z","shell.execute_reply.started":"2025-05-11T02:19:43.400328Z","shell.execute_reply":"2025-05-11T02:19:44.738986Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install accelerate","metadata":{"id":"mZVVXY8gZBuP","outputId":"29e25b31-fefd-4072-e410-58138bbe7340","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:19:44.740530Z","iopub.execute_input":"2025-05-11T02:19:44.740751Z","iopub.status.idle":"2025-05-11T02:20:57.735958Z","shell.execute_reply.started":"2025-05-11T02:19:44.740734Z","shell.execute_reply":"2025-05-11T02:20:57.735265Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:20:57.737713Z","iopub.execute_input":"2025-05-11T02:20:57.737926Z","iopub.status.idle":"2025-05-11T02:20:57.955782Z","shell.execute_reply.started":"2025-05-11T02:20:57.737908Z","shell.execute_reply":"2025-05-11T02:20:57.955137Z"}},"outputs":[{"name":"stdout","text":"Sun May 11 02:20:57 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   42C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install datasets","metadata":{"id":"d5a5yF_rZYCp","outputId":"8ab151f4-efc8-4919-b5fd-be0889f6a58e","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:20:57.956594Z","iopub.execute_input":"2025-05-11T02:20:57.956800Z","iopub.status.idle":"2025-05-11T02:21:05.122749Z","shell.execute_reply.started":"2025-05-11T02:20:57.956778Z","shell.execute_reply":"2025-05-11T02:21:05.121860Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install evaluate","metadata":{"id":"4aTdIHBfZHbT","outputId":"6efb5c16-fe63-4631-8fca-ca1b80cc9d63","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:05.123805Z","iopub.execute_input":"2025-05-11T02:21:05.124095Z","iopub.status.idle":"2025-05-11T02:21:08.541190Z","shell.execute_reply.started":"2025-05-11T02:21:05.124058Z","shell.execute_reply":"2025-05-11T02:21:08.540520Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import wandb\nimport os\n\n# Login using Kaggle secret\nwandb_api_key = os.environ.get('WANDB_API_KEY', None)\nif wandb_api_key is None:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nwandb.login(key=wandb_api_key)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:08.542144Z","iopub.execute_input":"2025-05-11T02:21:08.542363Z","iopub.status.idle":"2025-05-11T02:21:16.992790Z","shell.execute_reply.started":"2025-05-11T02:21:08.542341Z","shell.execute_reply":"2025-05-11T02:21:16.992092Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msyedmatherh789\u001b[0m (\u001b[33msyedmatherh789-habib-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')","metadata":{"id":"fZJ0XArpZVW6","outputId":"37723647-e3b0-4aea-a372-b12bfabaa68a","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:16.993628Z","iopub.execute_input":"2025-05-11T02:21:16.994047Z","iopub.status.idle":"2025-05-11T02:21:18.223896Z","shell.execute_reply.started":"2025-05-11T02:21:16.994027Z","shell.execute_reply":"2025-05-11T02:21:18.223292Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport os\nimport evaluate\nimport pandas as pd\nimport numpy as np\n\nif torch.cuda.is_available():\n    print(\"GPU is enabled.\")\n    print(\"device count: {}, current device: {}\".format(torch.cuda.device_count(), torch.cuda.current_device()))\nelse:\n    print(\"GPU is not enabled.\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #make sure GPU is enabled.\n\nimport accelerate\nimport transformers\nfrom datasets import load_dataset, DatasetDict, Dataset\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\nfrom transformers import DataCollatorForSeq2Seq\nfrom transformers import Seq2SeqTrainingArguments\nfrom torch.utils.data import DataLoader\nfrom transformers import Seq2SeqTrainer\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom torch.utils.data import DataLoader\n\nprint(transformers.__version__) #4.47.1\nprint(accelerate.__version__) #1.2.1","metadata":{"id":"RiDkSBQwZcYs","outputId":"d2605284-ae38-4322-865c-27cabb86f3b5","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:18.224628Z","iopub.execute_input":"2025-05-11T02:21:18.225315Z","iopub.status.idle":"2025-05-11T02:21:42.123201Z","shell.execute_reply.started":"2025-05-11T02:21:18.225296Z","shell.execute_reply":"2025-05-11T02:21:42.122527Z"}},"outputs":[{"name":"stderr","text":"2025-05-11 02:21:26.001989: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746930086.204398      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746930086.264136      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"GPU is enabled.\ndevice count: 2, current device: 0\n4.51.1\n1.3.0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Load Model and Tokenzier","metadata":{"id":"Q2svEMbtdGDD"}},{"cell_type":"code","source":"# model_checkpoint = 'csebuetnlp/mT5_multilingual_XLSum'\nmodel_checkpoint=\"google/flan-t5-base\"\nmodel_code = model_checkpoint.split(\"/\")[-1]\nmetric = evaluate.load(\"meteor\")","metadata":{"id":"3TtxJRzfcAKj","outputId":"31357f1b-400f-4786-c274-614609f22343","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:42.125824Z","iopub.execute_input":"2025-05-11T02:21:42.126571Z","iopub.status.idle":"2025-05-11T02:21:42.886381Z","shell.execute_reply.started":"2025-05-11T02:21:42.126549Z","shell.execute_reply":"2025-05-11T02:21:42.885640Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b32e4ee06471495ab7da491d922f3ab2"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\nconfig = AutoConfig.from_pretrained(\n    model_checkpoint,\n    max_length=128,\n    length_penalty=0.6,\n    no_repeat_ngram_size=2,\n    num_beams=15,\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, config=config).to(device)\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    return_tensors=\"pt\")\n\nprint(model_checkpoint)","metadata":{"id":"FlfQbeSAcH04","outputId":"c4ed210d-9a3d-45d4-a498-b90e4cf8ac30","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:42.887315Z","iopub.execute_input":"2025-05-11T02:21:42.887602Z","iopub.status.idle":"2025-05-11T02:21:48.965524Z","shell.execute_reply.started":"2025-05-11T02:21:42.887577Z","shell.execute_reply":"2025-05-11T02:21:48.964856Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f2e9431d5634d119fad7e86e1aff544"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b751554751a14ade85a91135c17957d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46d7ad6494894bb599c1c3c6be3280fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb5a42c668734a4db58d8853e8a75d29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ea14c16a61d41b995de258feb9d5092"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff7c9b0f0f0746cfbd59b4b479462cac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f459532e23247789885e541024e4142"}},"metadata":{}},{"name":"stdout","text":"google/flan-t5-base\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Prepare Data","metadata":{"id":"vYyc1Go0deba"}},{"cell_type":"code","source":"!pip install langdetect deep_translator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:48.966234Z","iopub.execute_input":"2025-05-11T02:21:48.966440Z","iopub.status.idle":"2025-05-11T02:21:54.683722Z","shell.execute_reply.started":"2025-05-11T02:21:48.966423Z","shell.execute_reply":"2025-05-11T02:21:54.682807Z"}},"outputs":[{"name":"stdout","text":"Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting deep_translator\n  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.3)\nRequirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\nDownloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=48be275eb6f2996bb860133ec940add7cfb99120bea092af31b6f1d97700db6d\n  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\nSuccessfully built langdetect\nInstalling collected packages: langdetect, deep_translator\nSuccessfully installed deep_translator-1.11.4 langdetect-1.0.9\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import re\nfrom langdetect import detect, DetectorFactory\nfrom deep_translator import GoogleTranslator\n\n# Set seed for reproducibility in langdetect\nDetectorFactory.seed = 0\n\n# Data preprocessing with cleaning\ndef preprocess_data(df):\n    def clean_text(text):\n        if pd.isna(text):\n            return \"\"\n        # Remove URLs and hashtags, retain some context for language detection\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n        text = re.sub(r'#\\w+', '', text)\n        text = text.strip()\n        return text\n\n    df['post'] = df['post'].apply(clean_text)\n    if 'normalized_claim' in df.columns:\n        df['normalized_claim'] = df['normalized_claim'].apply(clean_text)\n    return df\n\n# Function to clean and deduplicate text\ndef clean_text_deduplicate(text):\n    if pd.isna(text):\n        return \"\"\n    # Remove extra whitespace and normalize\n    text = ' '.join(text.split())\n    # Split into sentences and remove duplicates\n    sentences = text.split('. ')\n    sentences = [s.strip() for s in sentences if s.strip()]\n    unique_sentences = []\n    seen = set()\n    for s in sentences:\n        if s not in seen:\n            seen.add(s)\n            unique_sentences.append(s)\n    # Join back with proper punctuation\n    return '. '.join(unique_sentences) + ('.' if text.endswith('.') else '')\n\n# Function to detect language\n# Function to detect language (returns True if english, False otherwise)\ndef detect_language(text):\n    if pd.isna(text) or len(text.strip()) < 20:  # Increased threshold for short text\n        return True  # Assume english for short or empty texts\n    try:\n        detected_lang = detect(text)  # Store the result of detect(text)\n        return detected_lang == \"en\"\n    except:\n        return True  # Fallback to True (assume english) if detection fails\n\n\n# Function to translate text to english\ndef translate_to_english(text):\n    if pd.isna(text) or text.strip() == \"\":\n        return text\n    lang = detect_language(text)\n    if lang != 'en':\n        try:\n            translator = GoogleTranslator(source=lang, target='en')\n            translated_text = translator.translate(text)\n            return translated_text if translated_text else text\n        except Exception as e:\n            print(f\"Translation failed for text: {text}. Error: {e}\")\n            return text\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:54.684986Z","iopub.execute_input":"2025-05-11T02:21:54.685299Z","iopub.status.idle":"2025-05-11T02:21:54.970243Z","shell.execute_reply.started":"2025-05-11T02:21:54.685274Z","shell.execute_reply":"2025-05-11T02:21:54.969642Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/check-that-2025-task-2/clef2025-checkthat-lab-main-task2-data/task2/data/train/train-eng.csv\") # change path and file name\nval_data = pd.read_csv(\"/kaggle/input/check-that-2025-task-2/clef2025-checkthat-lab-main-task2-data/task2/data/dev/dev-eng.csv\")\n","metadata":{"id":"N_FyXSf6cls9","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:54.970921Z","iopub.execute_input":"2025-05-11T02:21:54.971867Z","iopub.status.idle":"2025-05-11T02:21:55.287219Z","shell.execute_reply.started":"2025-05-11T02:21:54.971847Z","shell.execute_reply":"2025-05-11T02:21:55.286627Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:55.287997Z","iopub.execute_input":"2025-05-11T02:21:55.288263Z","iopub.status.idle":"2025-05-11T02:21:55.293497Z","shell.execute_reply.started":"2025-05-11T02:21:55.288241Z","shell.execute_reply":"2025-05-11T02:21:55.292948Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(11374, 2)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"\n# Apply preprocessing\ntrain_data = preprocess_data(train_data)\n\n# Apply deduplication and further cleaning to 'post' column\ntrain_data['post'] = train_data['post'].apply(clean_text_deduplicate)\n\n# Apply deduplication and further cleaning to 'normalized_claim' column\ntrain_data['normalized claim'] = train_data['normalized claim'].apply(clean_text_deduplicate)\n\n# Filter out non-english posts with improved detection\n# Filter out non-english posts\n# Debug: Check how many posts are classified as english\ntrain_data['is_english'] = train_data['post'].apply(lambda x: detect_language(x) if pd.notna(x) else True)\nprint(\"Number of posts classified as english:\", train_data['is_english'].sum())\nprint(\"Sample of non-english posts (first 5):\")\nnon_english_samples = train_data[~train_data['is_english']].head(5)\nprint(non_english_samples[['post']])\n\n# Filter out non-english posts\ntrain_data = train_data[train_data['is_english']]\n\n# Remove the temporary 'is_english' column\ntrain_data = train_data.drop(columns=['is_english'])\n\n# Translate non-english posts to english\n# train_data['post'] = train_data['post'].apply(translate_to_english)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:21:55.294237Z","iopub.execute_input":"2025-05-11T02:21:55.294430Z","iopub.status.idle":"2025-05-11T02:22:25.685308Z","shell.execute_reply.started":"2025-05-11T02:21:55.294407Z","shell.execute_reply":"2025-05-11T02:22:25.684410Z"}},"outputs":[{"name":"stdout","text":"Number of posts classified as english: 10658\nSample of non-english posts (first 5):\n                                                 post\n3   Pa alam sayu idol🥺 Pa alam sayu idol🥺 Pa alam ...\n15  वीडियो को ज्यादा से ज्यादा शेयर करो जय श्री रा...\n35  गाय भारत की माता है, ऑस्ट्रेलिया में देखों कौन...\n63  Shhhhhhhhhh! Shhhhhhhhhh! Shhhhhhhhhh! THE FED...\n85  روسی صدر پیوٹن کا قوم سے اہم خطاب میں پاکستان ...\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"\n# Remove rows where both 'post' and 'normalized claim' are empty\ntrain_data = train_data[(train_data['post'] != '') | (train_data['normalized claim'] != '')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:25.686312Z","iopub.execute_input":"2025-05-11T02:22:25.686548Z","iopub.status.idle":"2025-05-11T02:22:25.694016Z","shell.execute_reply.started":"2025-05-11T02:22:25.686529Z","shell.execute_reply":"2025-05-11T02:22:25.693185Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:25.695061Z","iopub.execute_input":"2025-05-11T02:22:25.695310Z","iopub.status.idle":"2025-05-11T02:22:25.723249Z","shell.execute_reply.started":"2025-05-11T02:22:25.695287Z","shell.execute_reply":"2025-05-11T02:22:25.722649Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(10658, 2)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"train_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:25.723880Z","iopub.execute_input":"2025-05-11T02:22:25.724104Z","iopub.status.idle":"2025-05-11T02:22:25.743332Z","shell.execute_reply.started":"2025-05-11T02:22:25.724087Z","shell.execute_reply":"2025-05-11T02:22:25.742799Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                                    post  \\\n0      Lieutenant Retired General Asif Mumtaz appoint...   \n1      A priceless clip of 1970 of Bruce Lee playing ...   \n2      Hydrate YOURSELF W After Waking Up Water 30 mi...   \n4      Look how the media LIE TO STIR UP TROUBLE So a...   \n5      🚨 fact check 🚨Pfizer was not part of Donald Tr...   \n...                                                  ...   \n11369  Here in Bengal, an election rally of the Bhara...   \n11370  We urge employers to show flexibility to emplo...   \n11371  An ad being run in Georgia by Stephen Millerâ€...   \n11372  Joe Biden takes a knee after spotting an Ameri...   \n11373  Well we the conspiracy theorists, though we di...   \n\n                                        normalized claim  \n0      Pakistani government appoints former army gene...  \n1      Late actor and martial artist Bruce Lee playin...  \n2      Drinking water at specific times can have diff...  \n4      Kendall Jenner doctored a photo of her holding...  \n5             Pfizer is not part of Operation Warp Speed  \n...                                                  ...  \n11369  Here in Bengal, an election rally of the Bhara...  \n11370  Employers cannot penalise workers who are foll...  \n11371  “Kamala Harris said disaster aid should go to ...  \n11372  The claim: Presidential candidate Joe Biden kn...  \n11373  Microsoft developing human chips to store COVI...  \n\n[10658 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>post</th>\n      <th>normalized claim</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lieutenant Retired General Asif Mumtaz appoint...</td>\n      <td>Pakistani government appoints former army gene...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A priceless clip of 1970 of Bruce Lee playing ...</td>\n      <td>Late actor and martial artist Bruce Lee playin...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hydrate YOURSELF W After Waking Up Water 30 mi...</td>\n      <td>Drinking water at specific times can have diff...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Look how the media LIE TO STIR UP TROUBLE So a...</td>\n      <td>Kendall Jenner doctored a photo of her holding...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>🚨 fact check 🚨Pfizer was not part of Donald Tr...</td>\n      <td>Pfizer is not part of Operation Warp Speed</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11369</th>\n      <td>Here in Bengal, an election rally of the Bhara...</td>\n      <td>Here in Bengal, an election rally of the Bhara...</td>\n    </tr>\n    <tr>\n      <th>11370</th>\n      <td>We urge employers to show flexibility to emplo...</td>\n      <td>Employers cannot penalise workers who are foll...</td>\n    </tr>\n    <tr>\n      <th>11371</th>\n      <td>An ad being run in Georgia by Stephen Millerâ€...</td>\n      <td>“Kamala Harris said disaster aid should go to ...</td>\n    </tr>\n    <tr>\n      <th>11372</th>\n      <td>Joe Biden takes a knee after spotting an Ameri...</td>\n      <td>The claim: Presidential candidate Joe Biden kn...</td>\n    </tr>\n    <tr>\n      <th>11373</th>\n      <td>Well we the conspiracy theorists, though we di...</td>\n      <td>Microsoft developing human chips to store COVI...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10658 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"train_data = train_data.sample(frac=1).reset_index(drop=True)\nval_data = val_data.sample(frac=1).reset_index(drop=True)\n\nds = DatasetDict({\n        'train': Dataset.from_pandas(train_data),\n        'validation': Dataset.from_pandas(val_data)\n        })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:25.744140Z","iopub.execute_input":"2025-05-11T02:22:25.744363Z","iopub.status.idle":"2025-05-11T02:22:25.819637Z","shell.execute_reply.started":"2025-05-11T02:22:25.744346Z","shell.execute_reply":"2025-05-11T02:22:25.819115Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_data.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:25.820303Z","iopub.execute_input":"2025-05-11T02:22:25.820530Z","iopub.status.idle":"2025-05-11T02:22:25.825270Z","shell.execute_reply.started":"2025-05-11T02:22:25.820506Z","shell.execute_reply":"2025-05-11T02:22:25.824632Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(10658, 2)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"val_data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:25.826033Z","iopub.execute_input":"2025-05-11T02:22:25.826229Z","iopub.status.idle":"2025-05-11T02:22:25.836228Z","shell.execute_reply.started":"2025-05-11T02:22:25.826214Z","shell.execute_reply":"2025-05-11T02:22:25.835659Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(1171, 2)"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# Define tokenization function\ndef tokenize_sample_data(data):\n    # Max token size is set to 512 (reduced from 1024) and 128 for inputs and labels\n    input_feature = tokenizer(\n        data[\"post\"], truncation=True, max_length=512, padding=\"max_length\"\n    )\n    label = tokenizer(\n        data[\"normalized claim\"], truncation=True, max_length=128, padding=\"max_length\"\n    )\n    # Replace pad_token_id with -100 in labels to ignore padding in loss\n    labels = [l if l != tokenizer.pad_token_id else -100 for l in label[\"input_ids\"]]\n    return {\n        \"input_ids\": input_feature[\"input_ids\"],\n        \"attention_mask\": input_feature[\"attention_mask\"],\n        \"labels\": labels,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:25.836854Z","iopub.execute_input":"2025-05-11T02:22:25.837080Z","iopub.status.idle":"2025-05-11T02:22:25.845972Z","shell.execute_reply.started":"2025-05-11T02:22:25.837064Z","shell.execute_reply":"2025-05-11T02:22:25.845326Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Apply tokenization to datasets\nds = ds.map(tokenize_sample_data, batched=False)  # batched=False since the function expects single samples\n\n# Assign tokenized datasets\ntrain_dataset = ds['train']\neval_dataset = ds['validation']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:25.846662Z","iopub.execute_input":"2025-05-11T02:22:25.846880Z","iopub.status.idle":"2025-05-11T02:22:44.288492Z","shell.execute_reply.started":"2025-05-11T02:22:25.846856Z","shell.execute_reply":"2025-05-11T02:22:44.287831Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10658 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c1402cc290c45069113a94cb79a8f5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1171 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3646459a33114f52a14d8dd0819f4c6c"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# def tokenize_sample_data(data):\n#     # Max token size is set to 1024 and 128 for inputs and labels, respectively.\n#     input_feature = tokenizer(data[\"post\"], truncation=True, max_length=1024)\n#     label = tokenizer(data[\"normalized claim\"], truncation=True, max_length=128)\n#     return {\n#         \"input_ids\": input_feature[\"input_ids\"],\n#         \"attention_mask\": input_feature[\"attention_mask\"],\n#         \"labels\": label[\"input_ids\"],\n#     }\n\n\n# tokenized_ds = ds.map(\n#     tokenize_sample_data,\n#     remove_columns=[\"post\", \"normalized claim\"],\n#     batched=True,\n#     batch_size=1)\n\n\ndef tokenize_sentence(arg):\n    encoded_arg = tokenizer(arg)\n    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n\ndef metrics_func(eval_arg):\n    preds, labels = eval_arg\n    # Replace -100\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    # Convert id tokens to text\n    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    # Insert a line break (\\n) in each sentence for scoring\n    text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n    text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n    sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n    # compute METEOR score with custom tokenization\n    return metric.compute(\n        predictions=text_preds,\n        references=text_labels\n        )\n","metadata":{"id":"Vqrai6WqwGY2","outputId":"9b10ff0e-186b-49a0-dc5b-dfd19f85502f","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:44.289211Z","iopub.execute_input":"2025-05-11T02:22:44.289426Z","iopub.status.idle":"2025-05-11T02:22:44.296124Z","shell.execute_reply.started":"2025-05-11T02:22:44.289409Z","shell.execute_reply":"2025-05-11T02:22:44.295167Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Training","metadata":{"id":"CK02zo-bkQF1"}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir = f\"saved-models-{model_code}\",\n    num_train_epochs = 8,  # epochs\n    learning_rate = 5e-4,\n    lr_scheduler_type = \"linear\",\n    warmup_steps = 90,\n    optim = \"adamw_torch\",  #adam_hf was not a valid optimizer\n    weight_decay = 0.01,\n    per_device_train_batch_size = 1,\n    per_device_eval_batch_size = 1,\n    gradient_accumulation_steps = 4,\n    eval_steps = 100,\n    fp16=True,\n    predict_with_generate=True,\n    generation_max_length = 128,\n    logging_steps = 10,\n    push_to_hub = False,\n    save_strategy=\"steps\",\n    save_total_limit=2,         \n)\n\n\n\ntrainer = Seq2SeqTrainer(\n    model = model,\n    args = training_args,\n    data_collator = data_collator,\n    compute_metrics = metrics_func,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer = tokenizer\n)","metadata":{"id":"hX0ilEdk1OlK","outputId":"47ee9e6a-7b61-4dd2-b7c1-54d3bfc58c28","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:44.296999Z","iopub.execute_input":"2025-05-11T02:22:44.297224Z","iopub.status.idle":"2025-05-11T02:22:44.687905Z","shell.execute_reply.started":"2025-05-11T02:22:44.297204Z","shell.execute_reply":"2025-05-11T02:22:44.687154Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1275401652.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# trainer.evaluate()","metadata":{"id":"gKZAu_XsYavx","outputId":"5b631286-183b-4940-8a0d-4db6ba6fc7ec","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:44.688705Z","iopub.execute_input":"2025-05-11T02:22:44.688923Z","iopub.status.idle":"2025-05-11T02:22:44.692151Z","shell.execute_reply.started":"2025-05-11T02:22:44.688907Z","shell.execute_reply":"2025-05-11T02:22:44.691409Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# import gc\n# import torch\n\n# # After evaluation, clear memory before training\n# trainer.model.cpu()  # Move model temporarily to CPU\n# del trainer\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:44.696135Z","iopub.execute_input":"2025-05-11T02:22:44.696328Z","iopub.status.idle":"2025-05-11T02:22:44.705529Z","shell.execute_reply.started":"2025-05-11T02:22:44.696313Z","shell.execute_reply":"2025-05-11T02:22:44.704880Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, config=config).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:44.706320Z","iopub.execute_input":"2025-05-11T02:22:44.706569Z","iopub.status.idle":"2025-05-11T02:22:44.717790Z","shell.execute_reply.started":"2025-05-11T02:22:44.706542Z","shell.execute_reply":"2025-05-11T02:22:44.717222Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"trainer.train()\n\nos.makedirs(f\"{model_code}/finetuned_{model_code}\", exist_ok=True)\n\nif hasattr(trainer.model, \"module\"):\n    trainer.model.module.save_pretrained(f\"./{model_code}/finetuned_{model_code}\")\nelse:\n    trainer.model.save_pretrained(f\"./{model_code}/finetuned_{model_code}\")\n\nprint(\"Training done\")","metadata":{"id":"d3eJXWCnS5mt","outputId":"eb678371-2951-4838-f1d2-b8c0dfc8231c","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T02:22:44.718506Z","iopub.execute_input":"2025-05-11T02:22:44.718718Z","execution_failed":"2025-05-11T06:28:33.015Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250511_022245-ajkoks53</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/syedmatherh789-habib-university/huggingface/runs/ajkoks53' target=\"_blank\">saved-models-flan-t5-base</a></strong> to <a href='https://wandb.ai/syedmatherh789-habib-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/syedmatherh789-habib-university/huggingface' target=\"_blank\">https://wandb.ai/syedmatherh789-habib-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/syedmatherh789-habib-university/huggingface/runs/ajkoks53' target=\"_blank\">https://wandb.ai/syedmatherh789-habib-university/huggingface/runs/ajkoks53</a>"},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2100' max='10656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2100/10656 58:29 < 3:58:33, 0.60 it/s, Epoch 1.57/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.352500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.188700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.280400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.883100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.906500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.803600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.707700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.766100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.520300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.957400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.605900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.540300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>2.618200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.848500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.847100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.558000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>2.598300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.473900</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.786600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.527800</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>2.549500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>2.492900</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>2.661100</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>2.397100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.640600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>2.655400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.769200</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.506700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>2.271400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.575500</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>2.504800</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>2.653700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>2.533900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>2.580400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.457900</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.535900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>2.655700</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>2.503400</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>2.273100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.297800</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>2.189000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>2.500600</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>2.380000</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>2.526900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.574100</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>2.569200</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>2.378300</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>2.507600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>2.343200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.425600</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>2.387000</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>2.639400</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>2.524100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>2.408900</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>2.431400</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>2.561500</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>2.592700</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>2.202200</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>2.432000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.560200</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>2.251000</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>2.604000</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>2.328400</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>2.524200</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>2.377300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>2.317200</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>2.296100</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>2.438500</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>2.524000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.569300</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>2.294000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>2.394600</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>2.194800</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>2.209000</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>2.325100</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>2.397300</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>2.372200</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>2.343300</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>2.487900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.178900</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>2.350100</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>2.394600</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>2.451800</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>2.274500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>2.207700</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>2.366500</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>2.283100</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>2.049500</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>2.375000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.287200</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>2.208700</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>2.589800</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>2.260900</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>2.111400</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>2.238700</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>2.287400</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>2.196600</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>2.265800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.181100</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>2.321000</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>2.159600</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>2.110200</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>2.392300</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>2.069700</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>2.372500</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>2.205400</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>2.383200</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>2.285200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.415100</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>2.361400</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>2.233400</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>2.241300</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>2.173500</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>1.924700</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>2.057600</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>2.195400</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>2.234100</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>2.328400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.222400</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>2.326300</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>2.244400</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>2.408100</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>2.089400</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>2.352600</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>2.110100</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>2.033700</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>2.319300</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>2.130600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.280100</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>2.189000</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>2.288200</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>2.118800</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>1.867900</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>1.604800</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>1.718300</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>1.722200</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>1.548700</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>1.540400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.701700</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>1.704800</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>1.481100</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>1.376800</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>1.631000</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>1.547900</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>1.615600</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>1.519800</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>1.679200</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>1.805000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.654500</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>1.582800</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>1.814200</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>1.638900</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>1.912600</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>1.573600</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>1.622200</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>1.662100</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>1.588800</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>1.793100</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.770100</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>1.552200</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>1.572400</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>1.733400</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>1.499100</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>1.715400</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>1.789500</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>1.632600</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>1.588300</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>1.824000</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.423800</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>1.452900</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>1.770800</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>1.618400</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>1.423600</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>1.735100</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>1.602100</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>1.669000</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>1.496600</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>1.493000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.672000</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>1.633700</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>1.483700</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>1.439700</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>1.518600</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>1.686600</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>1.661900</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>1.635400</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>1.630400</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>1.505700</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.668500</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>1.568000</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>1.479000</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>1.590600</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>1.700800</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>1.587300</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>1.561300</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>1.615900</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>1.563900</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>1.686800</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.507800</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>1.743900</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>1.780500</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>1.615100</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>1.631100</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>1.590800</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>1.543500</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>1.318200</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>1.537600</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>1.523700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'num_beams': 15, 'length_penalty': 0.6, 'no_repeat_ngram_size': 2}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# loss= 0.524 ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract loss from the trainer's log history\nlogs = trainer.state.log_history\nloss_values = [log[\"loss\"] for log in logs if \"loss\" in log]\nsteps = [log[\"step\"] for log in logs if \"loss\" in log]\n\n# Plotting\nplt.figure(figsize=(10, 5))\nplt.plot(steps, loss_values, label=\"Training Loss\", color=\"blue\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Curve\")\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Save the plot to a file\nplt.savefig(\"Training loss_curve.png\", bbox_inches='tight', dpi=300)  # You can use .png, .jpg, .pdf, etc.\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create save directory if it doesn't exist\nsave_path = f\"./{model_code}/finetuned_{model_code}\"\nos.makedirs(save_path, exist_ok=True)\n\n# ✅ Save the tokenizer\ntokenizer.save_pretrained(save_path)\n\nprint(f\"✅ Model and tokenizer saved to {save_path}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport torch\n\ndel trainer\ndel model  # if declared separately\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from huggingface_hub.utils import validate_hf_hub_args\n# validate_hf_hub_args.repo_id = None  # clear repo_id forcibly if it's stored somewhere\n# # ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/saved-models-pegasus-xsum/checkpoint-5500\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Zip a folder\n# !zip -r finetuned_pegasus_xsum.zip /kaggle/working/pegasus-xsum/finetuned_pegasus-xsum\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Generate download link\n# from IPython.display import FileLink\n# FileLink(f\"/kaggle/working/finetuned_pegasus_xsum.zip\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# dataset_name = \"finetuned-pegasus-xsum-output\"\n# os.makedirs(\"my_dataset\", exist_ok=True)\n\n# # Move zip into dataset folder\n# !cp /kaggle/working/ffinetuned_pegasus_xsum.zip my_dataset/\n\n# # Create metadata\n# metadata = {\n#     \"title\": dataset_name,\n#     \"id\": \"atherhashmi/finetuned-pegasus-xsum-output\",\n#     \"licenses\": [{\"name\": \"CC0-1.0\"}]\n# }\n# import json\n# with open(\"my_dataset/dataset-metadata.json\", \"w\") as f:\n#     json.dump(metadata, f)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Path to your saved model\nmodel_path = f\"/kaggle/working/{model_code}/finetuned_{model_code}\"\n\n# Load the model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_args = Seq2SeqTrainingArguments(\n    output_dir=save_path,\n    per_device_eval_batch_size=1,\n    predict_with_generate=True,\n    generation_max_length=128,\n    logging_steps=10  # optional\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\neval_trainer = Seq2SeqTrainer(\n    model=model,\n    args=eval_args,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=metrics_func,\n    eval_dataset=eval_dataset\n)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_trainer.evaluate()","metadata":{"id":"ZQxlSbU6TDUc","outputId":"70fc3994-2188-4792-9f32-c93bd4c3aec4","trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inference function with prompting\ndef generate_normalized_claim(post):\n    prompt = f\"### Instruction: Summarize the following text by identifying the central assertion in a concise, factual statement:\\n\\n{post}\\n\\n### Response:\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs[\"input_ids\"],\n            max_new_tokens=120,\n            num_beams=5,\n            temperature=0.6,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id\n        )\n    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    response = prediction.split(\"### Response:\")[-1].strip()\n    torch.cuda.empty_cache()\n    return response","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test_df = preprocess_data(test_df)\n# test_df = test_df[test_df['language'].isin(valid_languages)]  # Filter for valid languages\n\n# # Generate predictions for test data\ndev_predictions = []\nfor post in tqdm(val_dataset['post'], desc=\"Generating predictions\",unit=\"post\"):\n    pred = generate_normalized_claim(post)\n    dev_predictions.append(pred)\n\n# # Save test predictions\nval_dataset['normalized claim'] = dev_predictions\ntest_df.to_csv(\"task2_dev_eng.csv\",index=False)\nprint(\"dev predictions saved to dev_predictions.csv\")\nprint(\"Sample test predictions:\")\nfor i in range(min(5, len(test_df))):\n    print(f\"Post: {val_dataset['post'].iloc[i]}\")\n    print(f\"Predicted Normalized Claim: {val_dataset['normalized claim'].iloc[i]}\\n\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/check-that-2025-task-2/clef2025-checkthat-lab-main-task2-data/task2/data/test/test-eng.csv\")\n#test_df = preprocess_data(test_df)\n# test_df = test_df[test_df['language'].isin(valid_languages)]  # Filter for valid languages\n\n# # Generate predictions for test data\ntest_predictions = []\nfor post in tqdm(test_df['post'], desc=\"Generating predictions\",unit=\"post\"):\n    pred = generate_normalized_claim(post)\n    test_predictions.append(pred)\n\n# # Save test predictions\ntest_df['normalized claim'] = test_predictions\ntest_df.to_csv(\"task2_eng.csv\",index=False)\nprint(\"Test predictions saved to test_predictions.csv\")\nprint(\"Sample test predictions:\")\nfor i in range(min(5, len(test_df))):\n    print(f\"Post: {test_df['post'].iloc[i]}\")\n    print(f\"Predicted Normalized Claim: {test_df['normalized claim'].iloc[i]}\\n\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.022Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**convert tsv to csv**","metadata":{}},{"cell_type":"code","source":"# import csv\n\n# # Change this path to where your file is mounted in Kaggle\n# tsv_path = \"/kaggle/working/task2_mono_english.tsv\"\n# csv_path = \"/kaggle/working/task2_eng.csv\"\n\n# # Convert TSV to CSV\n# with open(tsv_path, 'r', newline='', encoding='utf-8') as tsv_file, \\\n#      open(csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n\n#     tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n#     csv_writer = csv.writer(csv_file, delimiter=',')\n\n#     for row in tsv_reader:\n#         csv_writer.writerow(row)\n\n# print(f\"CSV file saved at: {csv_path}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# df= pd.read_csv(\"/kaggle/working/task2_eng.csv\")\n\n# df.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"4h31Kh0ekNKe"}},{"cell_type":"code","source":"# Load model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(f\"./{model_code}/finetuned_{model_code}\")\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"id":"7mYeSA1k1bsf","trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_text = \"Pence unfollowed Trump, and then changed his banner picture to Biden and Kamala. He’s outta there P...Syrian .\"\n\n# Tokenize the Input Text\ninputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():  # Disable gradient calculation\n    generated_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=5, early_stopping=True)\n\n# Decode the Generated Output\noutput_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n# Print the Output\nprint(f\"Generated Output: {output_text}\")","metadata":{"id":"0Ev0VWuK2zbL","trusted":true,"execution":{"execution_failed":"2025-05-11T06:28:33.024Z"}},"outputs":[],"execution_count":null}]}